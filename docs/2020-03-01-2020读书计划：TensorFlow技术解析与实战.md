---
layout: post
author: Train
description: 《TensorFlow技术解析与实战》读书笔记
keywords: TensorFlow
tags: [TensorFlow, thinking, reading]
---


春节假期准备消化一下几年前屯的书，于是随手挑了一本《TensorFlow技术解析与实战》。按照惯例，应该是完不成的；未料冠状病毒疫情意外延长了假期，竟得以囫囵看完了。小小激励之下，默默启动2020年读书计划——完成10本技术书籍的阅读和学习。此为第一篇。

我并不打算以章节笔记的形式来总结每本书的阅读，一方面懒于摘抄、照搬书上知识点，另一方面更习惯直接将注解、思考记录在书籍相应页面上。所以，这个系列更多的是读后感，包含以下两部分：

- `读后感` 全书宏观评价
- `故事梗概` 合上书后捋一遍以及此过程中印象深刻的点

## 读后感

一言以蔽之：内容丰富但流于表面，不适合新手入门，不适合老手深入。当时看得一头雾水体验及其不佳，好在内容尚且全面，并辅以其他在线资源的解惑，竟也带我入了门。

全书分为基础（TF基础，CNN/RNN基础）、实战（人脸识别、自然语言处理、语音识别、对抗网络）、提高（分布式、大数据、移动端）三篇，涉及内容甚广却仅仅300余页。内容丰富但流于表面，由此可见一斑。读完之后印象比较深的几点：

- 结构拼凑，没有作者的体悟和解读。很多基础知识点例如交叉熵损失函数、卷积网络需要读者自行查询详细资料。
- 代码堆砌，大量代码扩充篇幅却没有相应的解读。
- 校订不严谨，存在较多错别字、错误代码、错误图注。

客观来说，并不推荐此书作为TensorFlow或者深度学习的入门书籍。奈何我已经上了这条船，只能结合它尚算丰富的目录以及自己查阅相关资料后的注解，希望日后有用之处可以随手一翻。


## 故事梗概

- TensorFlow基础
    - TensorFlow编程架构 `Graph`和`Session`
    - MNIST数据集实战
        - `softmax`分类
        - `CNN`
        - `RNN`
        - 自编码器
        - 分布式训练
    - 该书基于TensorFlow 1.x，有此基础可以很快转向TensorFlow 2.x

- 神经网络基础
    - 激活函数：`sigmoid`、`tanh`、`relu`等
    - 卷积/池化函数：`padding`、`strikes`与卷积/池化后张量大小的计算
    - 分类相关损失函数：熵（信息量的期望）->散度熵（两个分布的差异）->交叉熵
        - `softmax`交叉熵 多分类互斥问题（`one-hot`编码）
        - `binary`交叉熵  二分类互斥，`softmax`交叉熵的特例
        - `sigmoid`交叉熵 多分类不互斥问题，每个特征看作一个`sigmoid`二分类问题，各个特征不互斥所以最终多分类
    - 优化算法：梯度下降算法
        - `BGD`、`SGD`
        - `Momentum`
        - `adadelta`
        - `RMSprop`
        - `adam`
    - 批标准化
    - L1、L2正则化

- 神经网络模型发展
    - 卷积神经网络`CNN`
        - LeNet：卷积和池化
        - AlexNet：`relu`引入、数据增强、大数据GPU
        - VggNet：标准化/模块化卷积/池化组
        - Inception：自行学习选择卷积核，并联3x3，5x5
        - ResNet：跨层获取输入以避免梯度消失
    - 循环神经网络`RNN`
        - RNN
        - LSTM

- 深度学习应用
    - 人脸识别
    - 自然语言处理
    - 语音识别
    - 生成对抗网络
    - 分布式
    - 大数据
    - 移动应用
